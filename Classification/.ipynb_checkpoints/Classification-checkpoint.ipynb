{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and predict knn on Modified congress voting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "# Create arrays for the features and the response variable\n",
    "y = df['party'].values\n",
    "X = df.drop('party', 1)\n",
    "\n",
    "# Create a k-NN classifier with 6 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Predict the labels for the training data X\n",
    "y_pred = knn.predict(X)\n",
    "\n",
    "# Predict and print the label for the new data point X_new\n",
    "new_prediction = knn.predict(X_new)\n",
    "print(\"Prediction: {}\".format(new_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset: digits\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Print the keys and DESCR of the dataset\n",
    "print(digits.keys())\n",
    "print(digits.DESCR)\n",
    "\n",
    "# Print the shape of the images and data keys\n",
    "print(digits.images.shape)\n",
    "print(digits.data.shape)\n",
    "\n",
    "# Display digit 1010\n",
    "plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit, predict and calculate accuracy using knn on mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create feature and target arrays\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a k-NN classifier with 7 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods of evaluating Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is mainly used to measure model performance in classification problems.\n",
    "\n",
    "Observe overfitting and underfitting by fitting models with differnt values for n_neighbours. Plot model accuracy scores on both training and testing data.\n",
    "\n",
    "However, accuracy is not always an informative metric.\n",
    "\n",
    "Class imbalance in a dataset makes accuracy as a measure of model performance useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup arrays to store train and test accuracies\n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neighbors):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "# Generate plot\n",
    "plt.title('k-NN: Varying Number of Neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Class Imbalance](class_imbalance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Confusion Matrix](confusion_matrix.png)\n",
    "\n",
    "![Metrics from aConfusion Matrix](confusion_matrix_metrics.png)\n",
    "\n",
    "A confusion matrix helps you get a better understanding of your model's performance.\n",
    "\n",
    "[Concusion matrix explanation](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit, predict and evaluate knn using confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit, predict and evaluate Logistic Regression (for Binary Classification) using confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Compute and print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves provide a way to visually evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Logistic Regression')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve to visually evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relationship between threshold(roc) and precision-recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the ROC curve.\n",
    "\n",
    "The bigger the area, the better the model.\n",
    "\n",
    "If the AUC is greater than 0.5, the model is better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: C. C controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large C can lead to an overfit model, while a small C can lead to an underfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "Aim is to use GridSearchCV and logistic regression to find the optimal C in this hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV\n",
    "\n",
    "GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold out set for final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hold out set for final evaluation](hold_out_set.png)\n",
    "\n",
    "You want to be absolutely certain about your model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to C, logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Instantiate the logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv = 5)\n",
    "\n",
    "# Fit it to the training data\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pipeline\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(pipeline, parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LogisticRegression and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9955456570155902\n",
    "\n",
    "0.9622222222222222\n",
    "\n",
    "1.0\n",
    "\n",
    "0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis for movie reviews\n",
    "\n",
    "In this exercise you'll explore the probabilities outputted by logistic regression on a subset of the Large Movie Review Dataset.\n",
    "\n",
    "The variables X and y are already loaded into the environment. X contains features based on the number of times words appear in the movie reviews, and y contains labels for whether the review sentiment is positive (+1) or negative (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "review2_features = get_features(review2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
    "    \n",
    "Probability of positive review: 0.8079007873616059\n",
    "    \n",
    "Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n",
    "    \n",
    "Probability of positive review: 0.5855117402793947 The second probability would have been even lower, but the word \"good\" trips it up a bit, since that's considered a \"positive\" word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Definitions](definitions.png)\n",
    "\n",
    "![Linear decision boundary](linear_decision_boundary.png)\n",
    "\n",
    "![linearly_separable_data](linearly_separable_data.png)\n",
    "\n",
    "In their basic forms LogisticRegression and SVC are linear classifiers i.e they learn linear decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing decision boundaries\n",
    "\n",
    "plot_4_classifers() function (similar to the code [here](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![decision_boundaries](decision_boundaries.svg)\n",
    "\n",
    "As you can see, logistic regression and linear SVM are linear classifiers whereas the default SVM and KNN are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dot_product](dot_product.png)\n",
    "\n",
    "![linear_classifier_prediction](linear_classifier_prediction.png)\n",
    "\n",
    "## How Logistic Regression makes predictions\n",
    "\n",
    "![how_lr_makes_predictions](how_lr_makes_predictions.png)\n",
    "\n",
    "## Visually look at prediction equation\n",
    "\n",
    "![example_decision_boundary](example_decision_boundary.png)\n",
    "\n",
    "Values of coefficients and intercepts determine the boundary.\n",
    "\n",
    "We change the intercept and get the below boundary.\n",
    "\n",
    "![intercept_decision_boundary](intercept_decision_boundary.png)\n",
    "\n",
    "To change the orientation of the boundary, we can change the coefficients.\n",
    "\n",
    "![coefficient_decision_boundary](coefficient_decision_boundary.png)\n",
    "\n",
    "\n",
    "Since logistic regression and SVMs are both linear classifiers, the raw model output is a linear function of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the model coefficients\n",
    "\n",
    "When you call fit with scikit-learn, the logistic regression coefficients are automatically learned from your dataset. In this exercise you will explore how the decision boundary is represented by the coefficients. To do so, you will change the coefficients manually (instead of with fit), and visualize the resulting classifiers.\n",
    "\n",
    "- Set the two coefficients and the intercept to various values and observe the resulting decision boundaries.\n",
    "- Try to build up a sense of how the coefficients relate to the decision boundary.\n",
    "- Set the coefficients and intercept such that the model makes no errors on the given training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the coefficients\n",
    "model.coef_ = np.array([[0,1]])\n",
    "model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![initial.svg](initial.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the coefficients\n",
    "model.coef_ = np.array([[-1,1]])\n",
    "model.intercept_ = np.array([-4])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final](final.svg)\n",
    "\n",
    "The coefficients determine the slope of the boundary and the intercept shifts it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Tells us how well/poorly our model is doing in the training set.\n",
    "\n",
    "**.fit** runs code that minimizes the loss.\n",
    "\n",
    "Classification uses loss functions that are different from regression loss functions.\n",
    "\n",
    "An example is the **no. of errors (0 1 Loss)**. 0 if prediction is correct, 1 if wrong. \n",
    "\n",
    "By summing the function over the number of training examples, we get the number of mistakes made in the training set.\n",
    "\n",
    "It's however very hard to minimize in practice hence not used by Logistic Regression and SVMs.\n",
    "\n",
    "**scipy.optimize.minimize** - for minimizing loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![least_squares](least_squares.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a loss function\n",
    "In this exercise you'll implement linear regression \"from scratch\" using scipy.optimize.minimize.\n",
    "\n",
    "We'll train a model on the Boston housing price data set, which is already loaded into the variables X and y. For simplicity, we won't include an intercept in our regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true-y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear_regression_loss](linear_regression_loss.png)\n",
    "\n",
    "![logistic_loss](logistic_loss.png)\n",
    "\n",
    "Logistic Loss is used by Logistic Regression.\n",
    "\n",
    "![hinge_loss](hinge_loss.png)\n",
    "\n",
    "Hinge Loss is used by Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the logistic and hinge losses\n",
    "\n",
    "![log_hinge](log_hinge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a plot of the logistic and hinge losses using their mathematical expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log_hinge_1](log_hinge_1.svg)\n",
    "\n",
    "These match up with the loss function diagrams we saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing logistic regression\n",
    "\n",
    "This is very similar to implementing linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(len(X)):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 1.03592182 -1.65378492  4.08331342 -9.40923002 -1.06786489  0.07892114\n",
    " -0.85110344 -2.44103305 -0.45285671  0.43353448]\n",
    " \n",
    "[[ 1.03731085 -1.65339037  4.08143924 -9.40788356 -1.06757746  0.07895582\n",
    "  -0.85072003 -2.44079089 -0.45271     0.43334997]]\n",
    "\n",
    "As you can see, logistic regression is just minimizing the loss function we've been looking at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Combats overfitting by punishing large model weights.\n",
    "\n",
    "Hyperparameter **c** is the inverse of the regularization strength i.e larger **c** means less regularization whereas smaller **c** means more regularization.\n",
    "\n",
    "regularized loss = original loss + coefficient penalty (high or low)\n",
    "\n",
    "Regularization:\n",
    "- reduces training error\n",
    "- improves test error\n",
    "\n",
    "### Logistic Regression with default regularization term\n",
    "![lr_default_regularization](lr_default_regularization.png)\n",
    "\n",
    "### Logistic Regression with high and low regularization (low c and high c)\n",
    "![lr_regularized](lr_regularized.png)\n",
    "\n",
    "### L1 VS L2 Regularization\n",
    "![l1_l2](l1_l2.png)\n",
    "\n",
    "![l2_l1_regularization](l2_l1_regularization.png)\n",
    "\n",
    "**L1 regularization** reduces coefficients to 0 hence perfoming feature selection.\n",
    "\n",
    "**L2 regularization** just shrinks the coefficients to be smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression\n",
    "\n",
    "We'll explore the effect of L2 regularization.\n",
    "\n",
    "- Loop over the different values of C_value, creating and fitting a LogisticRegression model each time.\n",
    "\n",
    "- Save the error on the training set and the validation set for each model.\n",
    "\n",
    "- Create a plot of the training and testing error as a function of the regularization parameter, C.\n",
    "\n",
    "- Looking at the plot, what's the best value of C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validaton errors initialized as empty list\n",
    "train_errs = list()\n",
    "valid_errs = list()\n",
    "\n",
    "# Loop over values of C_value\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C=C_value)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n",
    "    \n",
    "# Plot results\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_valid](train_valid.svg)\n",
    "\n",
    "As you can see, too much regularization (small C) doesn't work well - due to underfitting - and too little regularization (large C) doesn't work well either - due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and feature selection\n",
    "\n",
    "Perform feature selection on the movie review sentiment data set using L1 regularization.\n",
    "\n",
    "Search for the best value of C using scikit-learn's GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best CV params {'C': 1}\n",
    "    Total number of features: 2500\n",
    "    Number of selected features: 1220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the most positive and negative words\n",
    "\n",
    "We'll try to interpret the coefficients of a logistic regression fit on the movie review sentiment dataset.\n",
    "\n",
    "The words corresponding to the different features are loaded into the variable vocab. \n",
    "\n",
    "For example, since vocab[100] is \"think\", that means feature 100 corresponds to the number of times the word \"think\" appeared in that movie review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the sorted cofficients\n",
    "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
    "inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# Print the most positive words\n",
    "print(\"Most positive words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_descending[i]], end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print most negative words\n",
    "print(\"Most negative words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_ascending[i]], end=\", \")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most positive words: favorite, superb, noir, knowing, loved, \n",
    "\n",
    "Most negative words: disappointing, waste, worst, boring, lame,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and probabilities\n",
    "\n",
    "![lr_probabilites_unregularized](lr_probabilites_unregularized.png)\n",
    "\n",
    "The figure above shows the predicted probabilities of the red class. \n",
    "\n",
    "If we're more than 50% sure it's red, we predict red.\n",
    "\n",
    "If we're less than 50% sure it's red, we predict blue.\n",
    "\n",
    "We become more confident as we move away from the decision boundary.\n",
    "\n",
    "![lr_probabilities](lr_probabilities.png)\n",
    "\n",
    "Regularization makes the coefficients smaller.\n",
    "\n",
    "The probabilities are closer to 0.5 (We don't get to the very dark red and blue).\n",
    "\n",
    "This means we're less confident in our predictions.Hence preventing overconfidence/overfitting.\n",
    "\n",
    "Ratio of the coefficients gives us the slope of the line.\n",
    "\n",
    "Magnitude of the coefficients gives us the confidence level.\n",
    "\n",
    "Regularization affects the confidence and slope of boundary.\n",
    "\n",
    "![computing_probabilities](computing_probabilities.png)\n",
    "\n",
    "Sigmoid function used to squash raw model outputs to be between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and probabilities\n",
    "\n",
    "Observe the effects of changing the regularization strength on the predicted probabilities.\n",
    "\n",
    "- Compute the maximum predicted probability.\n",
    "- Run the provided code and take a look at the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", prob.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum predicted probability 0.9761229966765974\n",
    "\n",
    "![prob_1](prob_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a model with C=0.1 and examine how the plot and probabilities change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![prob_2](prob_2.svg)\n",
    "\n",
    "As you probably noticed, smaller values of C lead to less confident predictions. That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus, probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function. That's quite a chain of events!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing easy and difficult examples\n",
    "\n",
    "You'll visualize the examples that the logistic regression model is most and least confident about by looking at the largest and smallest predicted probabilities.\n",
    "\n",
    "The show_digit function takes in an integer index and plots the corresponding image, with some extra information displayed above the image.\n",
    "\n",
    "- Fill in the first blank with the index of the digit that the model is most confident about.\n",
    "- Fill in the second blank with the index of the digit that the model is least confident about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Get predicted probabilities\n",
    "proba = lr.predict_proba(X)\n",
    "\n",
    "# Sort the example indices by their maximum probability\n",
    "proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# Show the most confident (least ambiguous) digit\n",
    "show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# Show the least confident (most ambiguous) digit\n",
    "show_digit(proba_inds[0], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![most_confident](most_confident.svg)\n",
    "\n",
    "![least_confident](least_confident.svg)\n",
    "\n",
    "As you can see, the least confident example looks like a weird 4, and the most confident example looks like a very typical 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining binary classifier with one vs rest \n",
    "\n",
    "![one_vs_rest](one_vs_rest.png)\n",
    "\n",
    "Take the classifier that has the highest raw model output/ decision function.\n",
    "\n",
    "In the case above, it would be classifier 0 i.e it's more confident that the class is 0 than the other classes, so we predict class 0.\n",
    "\n",
    "One vs rest is the default behaviour of scikit learn's logistic regression. Consequently, the way to calculate loss (solver) is for the one vs rest case hence one needs to change this when moving to a multinomial approach. Additionally, one needs to add the multi-class parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial or softmax\n",
    "\n",
    "![one_rest_vs_multinomial](one_rest_vs_multinomial.png)\n",
    "\n",
    "Explanation:\n",
    "- Simple and modular: You can reuse your binary class implementation instead of creating a new one.\n",
    "- Tackle the problem directly: better accuracy since its loss is directly aligned with accuracy.\n",
    "\n",
    "\"Multinomial or softmax\" is standard in the field of neural networks.\n",
    "\n",
    "#### Counting the coefficients\n",
    "\n",
    "If you fit a logistic regression model on a classification problem with 3 classes and 100 features, how many coefficients would you have, including intercepts? **303**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting multi-class logistic regression\n",
    "\n",
    "Fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression()\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\")\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    OVR training accuracy: 0.9948032665181886\n",
    "    OVR test accuracy    : 0.9644444444444444\n",
    "    Softmax training accuracy: 1.0\n",
    "    Softmax test accuracy    : 0.9688888888888889\n",
    "    \n",
    "    As you can see, the accuracies of the two methods are fairly similar on this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing multi-class logistic regression\n",
    "\n",
    "In this exercise we'll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme.\n",
    "\n",
    "The data set is loaded into X_train and y_train. The two logistic regression objects,lr_mn and lr_ovr, are already instantiated (with C=100), fit, and plotted. As shown below.\n",
    "\n",
    "Notice that lr_ovr never predicts the dark blue class... yikes! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![softmax_1](softmax_1.svg)\n",
    "\n",
    "![ovr_1](ovr_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore why this happens by plotting one of the binary classifiers that it's using behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training accuracies\n",
    "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# Create the binary classifier (class 1 vs. rest)\n",
    "lr_class_1 = LogisticRegression(C=100)\n",
    "lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# Plot the binary classifier (class 1 vs. rest)\n",
    "plot_classifier(X_train, y_train==1, lr_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Softmax     training accuracy: 0.996\n",
    "    One-vs-rest training accuracy: 0.916\n",
    "\n",
    "![ovr_2](ovr_2.svg)\n",
    "\n",
    "As you can see, the binary classifier incorrectly labels almost all points in class 1 (shown as red triangles in the final plot)! \n",
    "\n",
    "Thus, this classifier is not a very effective component of the one-vs-rest classifier. In general, though, one-vs-rest often works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-rest SVM\n",
    "\n",
    "We'll repeat the previous exercise with a non-linear SVM.\n",
    "\n",
    "Instead of using LinearSVC, we'll now use scikit-learn's SVC object, which is a non-linear \"kernel\" SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use SVC instead of LinearSVC from now on\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create/plot the binary classifier (class 1 vs. rest)\n",
    "svm_class_1 = SVC()\n",
    "svm_class_1.fit(X_train, y_train==1)\n",
    "plot_classifier(X_train, y_train==1, svm_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svc](svc.svg)\n",
    "\n",
    "The non-linear SVM works fine with one-vs-rest on this dataset because it learns to \"surround\" class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vectors\n",
    "\n",
    "![svm](svm.png)\n",
    "\n",
    "![svm_ctd](svm_ctd.png)\n",
    "\n",
    "![max_margin](max_margin.png)\n",
    "\n",
    "If the regularization strength is not too large, SVM's maximize the margins of linearly separable datasets.\n",
    "\n",
    "Which of the following is a true statement about support vectors? \n",
    "1. All support vectors are classified correctly.\n",
    "2. All support vectors are classified incorrectly.\n",
    "3. All correctly classified points are support vectors.\n",
    "4. All incorrectly classified points are support vectors.\n",
    "\n",
    "Answer: **4** (Also points close to the decision boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of removing examples\n",
    "\n",
    "**Support vectors** are defined as training examples that influence the decision boundary. \n",
    "\n",
    "In this exercise, you'll observe this behavior by removing non support vectors from the training set.\n",
    "\n",
    "- Train a linear SVM on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X, y)\n",
    "plot_classifier(X, y, svm, lims=(11,15,0,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm_1](svm_1.svg)\n",
    "\n",
    "- Create a new data set containing only the support vectors.\n",
    "- Train a new linear SVM on the smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small, y_small)\n",
    "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm_2](svm_2.svg)\n",
    "\n",
    "    Number of original examples 178\n",
    "    Number of support vectors 81\n",
    "    \n",
    "Compare the decision boundaries of the two trained models: are they the same? By the definition of support vectors, they should be!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM\n",
    "\n",
    "![feature_transformation](feature_transformation.png)\n",
    "\n",
    "![transformation_decision_boundaries](transformation_decision_boundaries.png)\n",
    "\n",
    "Fitting a linear model in a transformed space is the same as fitting a non-linear model in an untransformed space.\n",
    "\n",
    "**Kernel SVM** performs feature transformation in a computationally efficient way.\n",
    "\n",
    "You can control the shape of the boundary using hyperparameters.\n",
    "\n",
    "**C** - controls regularization\n",
    "\n",
    "**kernel** - default is **rbf(radial basis function)**\n",
    "\n",
    "**gamma** - influence smoothness of boundary:\n",
    "- decreasing the value leads to a smoother boundary\n",
    "- a larger values for gamma lead to more complex decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV warm-up\n",
    "We saw that increasing the RBF kernel hyperparameter gamma increases training accuracy. \n",
    "\n",
    "In this exercise we'll search for the gamma that maximizes cross-validation accuracy using scikit-learn's GridSearchCV. \n",
    "\n",
    "A binary version of the handwritten digits dataset, in which you're just trying to predict whether or not an image is a \"2\", is already loaded into the variables X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X, y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV params {'gamma': 0.001}\n",
    "\n",
    "Larger values of gamma are better for training accuracy, but cross-validation helped us find something different (and better!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointly tuning gamma and C with GridSearchCV\n",
    "In the previous exercise the best value of gamma was 0.001 using the default value of C, which is 1. In this exercise you'll search for the best combination of C and gamma using GridSearchCV.\n",
    "\n",
    "As in the previous exercise, the 2-vs-not-2 digits dataset is already loaded, but this time it's split into the variables X_train, y_train, X_test, and y_test. Even though cross-validation already splits the training set into parts, it's often a good idea to hold out a separate test set to make sure the cross-validation results are sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best CV params {'C': 10, 'gamma': 0.0001}\n",
    "    Best CV accuracy 0.9988864142538976\n",
    "    Test accuracy of best grid search hypers: 0.9988876529477196\n",
    "\n",
    "Note that the best value of gamma, 0.0001, is different from the value of 0.001 that we got in the previous exercise, when we fixed C=1. Hyperparameters can affect each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Logistic Regression and SVM (and beyond)\n",
    "\n",
    "![logistic_svm](logistic_svm.png)\n",
    "\n",
    "**Advantage of Logistic Regression over SVM?** \n",
    "It naturally outputs meaningful probabilities.\n",
    "\n",
    "**Advantage of SVM over Logistic Regression?**\n",
    "It is faster when used with kernels. Having a limited number of support vectors makes kernel SVMs computationally efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier\n",
    "\n",
    "![SGDClassifier](SGDClassifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGDClassifier\n",
    "\n",
    "You'll do a hyperparameter search over the regularization type, regularization strength, and the loss (logistic regression vs. linear SVM) using SGDClassifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set random_state=0 for reproducibility \n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss':['hinge', 'log'], 'penalty':['l1', 'l2']}\n",
    "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Best CV params {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1'}\n",
    "    Best CV accuracy 0.94351630867144\n",
    "    Test accuracy of best grid search hypers: 0.9592592592592593\n",
    "    \n",
    "**One advantage of SGDClassifier is that:** it's very fast - this would have taken a lot longer with LogisticRegression or LinearSVC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
