{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "A predictive model that investigates the relationship\n",
    "between the dependent-target variable and\n",
    "independent-regressor variable(s).\n",
    "\n",
    "The regressor fits the best line/curve to the dataset to\n",
    "capture the above relationship.\n",
    "\n",
    "Regression is used in forecasting, time series modelling\n",
    "and other areas where the relationship among variables\n",
    "is key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and reshape e.g y=price x=rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data and get it into the form needed by scikit-learn. This involves creating feature and target variable arrays. Furthermore, since you are going to use only one feature to begin with, you need to do some reshaping using NumPy's .reshape() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame: df\n",
    "df = pd.read_csv('gapminder.csv')\n",
    "\n",
    "# Create arrays for features and target variable\n",
    "y = df['life'].values\n",
    "X = df['fertility'].values\n",
    "\n",
    "# Print the dimensions of X and y before reshaping\n",
    "print(\"Dimensions of y before reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X before reshaping: {}\".format(X.shape))\n",
    "\n",
    "# Reshape X and y\n",
    "y = y.reshape(-1, 1)\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# Print the dimensions of X and y after reshaping\n",
    "print(\"Dimensions of y after reshaping: {}\".format(y.shape))\n",
    "print(\"Dimensions of X after reshaping: {}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "In this type of predictive model:\n",
    "- The dependent variable is continuous in nature.\n",
    "- There exists only one independent variable.\n",
    "\n",
    "The regressor fits a straight line in the dataset (Line of best fit).\n",
    "\n",
    "The line of best fit has the equation **y = mx + c** where\n",
    "**m** − slope or gradient and **c** − y intercept.\n",
    "\n",
    "To fit the best line to our dataset we need to find the\n",
    "best slope **m** and the best intercept **c**.\n",
    "\n",
    "It is because of this that we define an **error function (loss function)** that\n",
    "measures how “good” our line is in every iteration/epoch.\n",
    "\n",
    "In every iteration we shall seek to minimize the **error (loss function)**\n",
    "thus making the line better and better. This is how the\n",
    "linear regressor learns.\n",
    "\n",
    "To minimize the loss function we need to find its minima\n",
    "values.\n",
    "\n",
    "Minima of a function can be calculated by computing its derivative with respect to its independent variables.\n",
    "\n",
    "**Gradient Descent** is a method than can be used to\n",
    "efficiently find minima through convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "It is an iterative optimization algorithm used to find the\n",
    "minimum value for a high-dimensional function.\n",
    "\n",
    "It moves in a given direction and takes a magnitude of\n",
    "steps to try and hit a minimum value in the function.\n",
    "\n",
    "The magnitude of the steps are defined using the\n",
    "**learning rate - α**.\n",
    "\n",
    "We can cover more area with larger **steps/higher\n",
    "learning rate** but are at the risk of overshooting the\n",
    "minima. May fail to converge. Leading to accuracy issues.\n",
    "\n",
    "On the other hand, **small steps/smaller learning rates**\n",
    "will consume a lot of time to reach the lowest point. Take a long time to converge. Leading to efficiency issues.\n",
    "\n",
    "If we are able to compute the derivative of a function, we know in\n",
    "which direction to proceed to minimize it.\n",
    "\n",
    "Optimization algorithms like gradient descent use derivates to\n",
    "actually decide whether to increase or decrease the weights in\n",
    "order to increase or decrease any objective function.\n",
    "\n",
    "Example, if we are moving upwards, we are moving away from the\n",
    "minima and vice versa\n",
    "We thus have to move downhill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Descent](gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do I pick a learning rate?\n",
    "\n",
    "Plot the cost function against different values of **α** and\n",
    "pick the value of **α** that is right before the first value that\n",
    "didn’t converge so that we would have a very fast\n",
    "learning algorithm that converges – balance accuracy &\n",
    "efficiency.\n",
    "\n",
    "From this technique; The most commonly used rates are : 0. 001, 0. 003, 0. 01, 0. 03, 0. 1, 0. 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Learning Rate](learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Working of Linear Regression](regression_mechanics.png)\n",
    "\n",
    "![Linear Regression Loss function](regression_loss_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals represent the vertical distances between the points and the line.\n",
    "\n",
    "A large +ve residual cancels out a large -ve residual hence the need to do mean of the square of the residual. \n",
    "\n",
    "This is similar to mean squared error of the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss Functions\n",
    "\n",
    "### 1. Mean Absolute Error (MAE)\n",
    "\n",
    "### 2. Mean Square Error (MSE)\n",
    "\n",
    "### 3. Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of simple Linear Regression\n",
    "Implemention of [Simple Linear Regression](https://github.com/aubreyomondi/ML_Practicals/blob/master/Linear_Regression/linear_regression_GD.ipynb) from scratch i.e without use of libraries.\n",
    "\n",
    "Implemention of [Simple Linear Regression](https://github.com/aubreyomondi/ML_Practicals/blob/master/Linear_Regression/linear_regression_GD.ipynb) using Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "In this type of predictive model:\n",
    "- The dependent variable is continuous in nature.\n",
    "- There exists more than one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multiple Linear Regression](regression_higher_dimen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure model performance in Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R^2 (\"R Squared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# Create the regressor: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Create the prediction space\n",
    "prediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1,1)\n",
    "\n",
    "# Fit the model to the data\n",
    "reg.fit(X_fertility, y)\n",
    "\n",
    "# Compute predictions over the prediction space: y_pred\n",
    "y_pred = reg.predict(prediction_space)\n",
    "\n",
    "# Print R^2 \n",
    "print(reg.score(X_fertility, y))\n",
    "\n",
    "# Plot regression line\n",
    "plt.plot(prediction_space, y_pred, color='black', linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r2, mse, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit the regressor to the training data\n",
    "reg_all.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(X_test)\n",
    "\n",
    "# Compute and print R^2\n",
    "print(\"R^2: {}\".format(reg_all.score(X_test, y_test)))\n",
    "\n",
    "# Compute and print MSE\n",
    "print(\"MSE: {}\".format(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Compute and print RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a vital step in evaluating a model. \n",
    "\n",
    "It maximizes the amount of data that is used to train the model, as during the course of training, the model is not only trained, but also tested on all of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(reg, X, y, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check computation time for cross validation with different folds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Perform 3-fold CV\n",
    "%timeit cvscores_3 = cross_val_score(reg, X, y, cv=3)\n",
    "print(np.mean(cvscores_3))\n",
    "\n",
    "# Perform 10-fold CV\n",
    "%timeit cvscores_10 = cross_val_score(reg, X, y, cv=10)\n",
    "print(np.mean(cvscores_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "**Regularization**: any modification we make to a learning\n",
    "algorithm that is intended to reduce its generalization\n",
    "error but not its training error.\n",
    "\n",
    "Helps in avoiding overfitting by penalizing large weights i.e.\n",
    "down weighting features that are unlikely to generalize\n",
    "well.\n",
    "\n",
    "Regularization:\n",
    "- L1 Regularization: Lasso Regression\n",
    "- L2 Regularization: Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Over and Under Fitting](over_under_fitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regularization](regularization.png)\n",
    "\n",
    "![Lasso Regression](lasso_regression.png)\n",
    "\n",
    "![Lasso Regression for feature selection](lasso_feature_selection.png)\n",
    "\n",
    "![Ridge Regression](ridge_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization/Lasso Regression\n",
    "\n",
    "On L1 Loss function(Mean Absolute Error)\n",
    "\n",
    "lasso performs regularization by adding to the loss function a penalty term of the absolute value of each coefficient multiplied by some alpha. This is also known as L1 regularization because the regularization term is the L1 norm of the coefficients.\n",
    "\n",
    "L1 regularization is a linear function of the weight values\n",
    "\n",
    "Named after the L1 norm, the sum of the absolute values of\n",
    "the weights, or Manhattan distance\n",
    "\n",
    "We optimize the L1 regularized Loss function. We do this by calculating partial derivatives and updating the weights in each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Instantiate a lasso regressor: lasso\n",
    "lasso = Lasso(alpha=0.4, normalize=True)\n",
    "\n",
    "# Fit the regressor to the data\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.plot(range(len(df_columns)), lasso_coef)\n",
    "plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\n",
    "plt.margins(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization/Ridge Regression\n",
    "\n",
    "On L2 Loss function(Mean Square Error)\n",
    "\n",
    "If instead you took the sum of the squared values of the coefficients multiplied by some alpha - like in Ridge regression - you would be computing the L2 norm.\n",
    "\n",
    "L2 regularization is a quadratic function of the weight values\n",
    "\n",
    "Named that way because it uses the (square of the) L2 norm of the weight\n",
    "values.\n",
    "\n",
    "The L2 norm, is the same as the Euclidean distance\n",
    "\n",
    "We optimize the L2 regularized Loss function. We do this by calculating partial derivatives and updating the weights in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate ridge scores for different alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Setup the array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemention of Ridge Regression\n",
    "Implemention of [Ridge Regression](https://github.com/johnGachihi/Machine-Learning/blob/master/LINEAR_REGRESSION_WITH_L2.ipynb) from scratch i.e without use of libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 VS L2 Regularization\n",
    "\n",
    "L2 regularization is easier to optimize because of its simple\n",
    "derivative (the derivative of w2 is just 2w)\n",
    "\n",
    "while L1 regularization is more complex (and the derivative of |w|\n",
    "is noncontinuous at zero).\n",
    "\n",
    "But where L2 prefers weight vectors with many small weights\n",
    "\n",
    "L1 prefers “sparse” weights with some larger weights but many\n",
    "more weights set to zero.\n",
    "\n",
    "Thus L1 regularization leads to much sparser weight vectors, i.e. , far\n",
    "fewer features – feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regularization\n",
    "\n",
    "In elastic net regularization, the penalty term is a linear combination of the L1 and L2 penalties:\n",
    "\n",
    "a∗L1+b∗L2\n",
    "\n",
    "In scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an L1 penalty, and anything lower is a combination of L1 and L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('elasticnet', ElasticNet())]\n",
    "\n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(pipeline, parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print the metrics\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
